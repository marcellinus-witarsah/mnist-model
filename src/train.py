# This is a sample Python script.

# Press Shift+F10 to execute it or replace it with your code.
# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.
import tensorflow as tf
import PIL
import pandas as pd
import numpy as np
import tensorflow_datasets as tfds
import scipy
import matplotlib.pyplot as plt

BUFFER_SIZE = 42
BATCH_SIZE = 32


def check_version(library):
    if library is None:
        print("no library is inputted")
        return

    if library.__name__ == "tensorflow":
        print("GPU is available, Device = {}".format(tf.config.list_physical_devices("GPU"))
              if tf.config.list_physical_devices("GPU") else "GPU is not available")
    print("{} version is {}".format(library.__name__, library.__version__))


def get_model_checkpoint(filepath, monitor, mode):
    """
    :param filepath: location of the checkpoint
    :param monitor: metric that is being monitored
    :param mode: min/ max
    :return: ModelCheckpoint class
    """
    return tf.keras.callbacks.ModelCheckpoint(
        filepath=filepath,
        monitor=monitor,
        mode=mode,
        save_best_only=True,
        verbose=2)


def get_early_stopping(monitor, patience):
    """
    :param monitor: metric that is being monitored
    :param patience: how many epoch for stopping the model training if the metric or loss is not inmproving
    :return: EarlyStopping class
    """
    return tf.keras.callbacks.EarlyStopping(
      monitor=monitor,
      patience=patience,
      verbose=2
    )


def normalize_image(image, label):
    """
    :param image: image/feature of the data
    :param label: label of the image
    :return: normalize image and label
    """
    return tf.cast(image, tf.float32) / 255.0, label


def get_data():
    """
    :return: prefetch train dataset, prefetch validation dataset, dataset info
    """
    # extract datasets
    (train_ds, val_ds), info_ds = tfds.load(
        'mnist',
        as_supervised=True,
        split=['train', 'test'],
        with_info=True,
        shuffle_files=True,
    )

    # transform datasets
    preprocessed_train_ds = train_ds.map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE).cache()
    preprocessed_val_ds = val_ds.map(normalize_image, num_parallel_calls=tf.data.AUTOTUNE).cache()

    shuffled_batched_train_ds = preprocessed_train_ds\
        .shuffle(info_ds.splits["train"].num_examples)\
        .batch(BATCH_SIZE)
    batched_val_ds = preprocessed_val_ds\
        .batch(BATCH_SIZE)

    prefetched_train_ds = shuffled_batched_train_ds.prefetch(tf.data.experimental.AUTOTUNE)
    prefetched_val_ds = batched_val_ds.prefetch(tf.data.experimental.AUTOTUNE)

    return prefetched_train_ds, prefetched_val_ds, info_ds


def get_cnn_model(input_shape):
    """
    :param input_shape: the shape of an input image
    :return: CNN model
    """
    model = tf.keras.models.Sequential(
        [
            tf.keras.layers.Conv2D(
                filters=16,
                input_shape=input_shape,
                kernel_size=(3, 3),
                activation=tf.nn.relu
            ),
            tf.keras.layers.MaxPool2D(
                pool_size=(2, 2),
            ),
            tf.keras.layers.Conv2D(
                filters=32,
                kernel_size=(3, 3),
                activation=tf.nn.relu
            ),
            tf.keras.layers.MaxPool2D(
                pool_size=(2, 2),
            ),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(units=64, activation=tf.nn.relu),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)
        ]
    )
    return model


if __name__ == '__main__':
    libraries = [tf, PIL, pd, np, tfds, scipy]
    for library in libraries:
        check_version(library=library)

    # GET DATA
    ds_train, ds_test, ds_info = get_data()
    input_shape = np.array(ds_info.features['image'].shape)

    for img in iter(ds_train.take(1)):
        print(np.shape(img))
        # plt.imshow(img, cmap="Greys")
        # plt.show()

    # # RESET ALL STATE GENERATED BY KERAS
    # tf.keras.backend.clear_session()
    # # CREATE CNN MODEL
    # model = get_cnn_model(input_shape)
    # # COMPILE MODEL
    # model.compile(
    #     optimizer=tf.keras.optimizers.Adam(0.001),
    #     loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    #     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
    # )

    # # SETTING UP FOLDER PATH AND FOLDER NAME
    # folder_name = 'best_1'
    # folder_path = os.path.join(os.getcwd(), folder_name)
    # # CHECKING THE FOLDER IF EXIST, IF NOT CREATE THE FOLDER
    # if not os.path.exists(folder_path):
    #     os.mkdir(folder_path)
    #     print("{} folder is created".format(folder_name))
    # else:
    #     print("{} folder has already existed".format(folder_name))
    #
    # # CREATE CALLBACKS
    # callbacks = [
    #     get_model_checkpoint(os.path.join(folder_path, 'model'), monitor='val_sparse_categorical_accuracy', mode='max'),
    #     get_early_stopping(monitor='val_loss', patience=5)
    # ]
    #
    # history = model.fit(
    #     ds_train,
    #     epochs=50,
    #     steps_per_epoch=ds_info.splits['train'].num_examples // BATCH_SIZE,
    #     validation_data=ds_test,
    #     validation_steps=ds_info.splits['test'].num_examples // BATCH_SIZE,
    #     callbacks=callbacks
    # )
    #
    # # SAVE ALL METRICS AND LOSS IN FORM OF .CSV FILE
    # history_pd = pd.DataFrame(history.history)
    # history_csv_file = os.path.join(folder_path, 'history.csv')
    # with open(history_csv_file, mode='w') as f:
    #     history_pd.to_csv(f)

